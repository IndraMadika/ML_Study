{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C4_W1_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IndraMadika/ML_Study/blob/main/C4_W1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXdEnmVvXdUa"
      },
      "source": [
        "# Train Your Own Model and Serve It With TensorFlow Serving\n",
        "\n",
        "In this notebook, you will train a neural network to classify images of handwritten digits from the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. You will then save the trained model, and serve it using [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfb9Qd5XdFL"
      },
      "source": [
        "**Warning: This notebook is designed to be run in a Google Colab only**.  It installs packages on the system and requires root access. If you want to run it in a local Jupyter notebook, please proceed with caution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Q8bkjeYTl-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8r89tTPI-Kb",
        "outputId": "99cd9422-f3cc-4ccd-8465-4bb60fe2c86a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGFJmWjrKttn",
        "outputId": "238fac0e-eaf4-4f84-d401-e671ad3c3106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â€¢ Using TensorFlow Version: 2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq-214o8SNt0"
      },
      "source": [
        "## Import the MNIST Dataset\n",
        "\n",
        "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains 70,000 grayscale images of the digits 0 through 9. The images show individual digits at a low resolution (28 by 28 pixels). \n",
        "\n",
        "Even though these are really images, we will load them as NumPy arrays and not as binary image objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqDQO0KCaWS",
        "outputId": "f249496a-51ea-40ac-e605-59c215095a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIT-qX0QzLo-"
      },
      "source": [
        "# EXERCISE: Scale the values of the arrays below to be between 0.0 and 1.0.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 225.0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIDGu-EEzdKb"
      },
      "source": [
        "In the cell below use the `.reshape` method to resize the arrays to the following sizes:\n",
        "\n",
        "```python\n",
        "train_images.shape: (60000, 28, 28, 1)\n",
        "test_images.shape: (10000, 28, 28, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsIxeG6BzN4t"
      },
      "source": [
        "# EXERCISE: Reshape the arrays below.\n",
        "train_images = train_images.reshape(train_images.shape[0], 28,28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUw8ZxigB1Nx",
        "outputId": "46dd87e8-ba28-41d2-a154-75f03cba8623",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
        "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train_images.shape: (60000, 28, 28, 1), of float64\n",
            "test_images.shape: (10000, 28, 28, 1), of float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcR0OKbOSj0c"
      },
      "source": [
        "## Look at a Sample Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMs4v_oSo9v",
        "outputId": "bc458119-911e-42f7-e8fa-4006a46c83b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "idx = 42\n",
        "\n",
        "plt.imshow(test_images[idx].reshape(28,28), cmap=plt.cm.binary)\n",
        "plt.title( {}'True Label:'.format(test_labels[idx]), fontdict={'size': 16})\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn_-9OsPYnDp"
      },
      "source": [
        "## Build a Model\n",
        "\n",
        "In the cell below build a `tf.keras.Sequential` model that can be used to classify the images of the MNIST dataset. Feel free to use the simplest possible CNN. Make sure your model has the correct `input_shape` and the correct number of output units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgMgJJynMbVY",
        "outputId": "1305d287-1298-4abd-8297-4c94ab6756df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# EXERCISE: Create a model.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(input_shape=(28,28, 1), filters=8, kernel_size=3,\n",
        "                           strides=2, activation='relu', name='Conv1'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
        "\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv1 (Conv2D)              (None, 13, 13, 8)         80        \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1352)              0         \n",
            "                                                                 \n",
            " Softmax (Dense)             (None, 10)                13530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,610\n",
            "Trainable params: 13,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLzXnZT1YvS6"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "In the cell below configure your model for training using the `adam` optimizer, `sparse_categorical_crossentropy` as the loss, and `accuracy` for your metrics. Then train the model for the given number of epochs, using the `train_images` array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTNN0ANGgA36",
        "outputId": "0069b51a-4a6e-48ca-bb54-ee5909fb92d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# EXERCISE: Configure the model for training.\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# EXERCISE: Train the model.\n",
        "history = model.fit(train_images, train_labels, epochs=epochs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 14s 3ms/step - loss: 2.3016 - accuracy: 0.1113\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 2.3013 - accuracy: 0.1124\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 2.3014 - accuracy: 0.1124\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er_vrDhf4qu5"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMD387B93f2g",
        "outputId": "b89990b8-6cf5-4c87-ccbf-925ffaa45e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# EXERCISE: Evaluate the model on the test images.\n",
        "results_eval = model.evaluate(test_images, test_labels, verbose=0)\n",
        "for metric, value in zip(model.metrics_names, results_eval):\n",
        "    print(metric + ': {:.3}'.format(value))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.3\n",
            "accuracy: 0.113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGfmT8M1Yx5y"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uFDoDW_7HX6",
        "outputId": "718328e4-e0cd-4b3d-e30a-d2a5e4491c0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MODEL_DIR = tempfile.gettempdir()\n",
        "\n",
        "version = 1\n",
        "\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "\n",
        "if os.path.isdir(export_path):\n",
        "    print('\\nAlready saved a model, cleaning up\\n')\n",
        "    !rm -r {export_path}\n",
        "\n",
        "model.save(export_path, save_format=\"tf\")\n",
        "\n",
        "print('\\nexport_path = {}'.format(export_path))\n",
        "!ls -l {export_path}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) Conv1_input with unsupported characters which will be renamed to conv1_input in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "export_path = /tmp/1\n",
            "total 112\n",
            "drwxr-xr-x 2 root root  4096 May 12 09:41 assets\n",
            "-rw-r--r-- 1 root root    55 May 12 09:41 fingerprint.pb\n",
            "-rw-r--r-- 1 root root  8612 May 12 09:41 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 89982 May 12 09:41 saved_model.pb\n",
            "drwxr-xr-x 2 root root  4096 May 12 09:41 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KziE3e9tY-hH"
      },
      "source": [
        "## Examine Your Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU4GDF_aYtfQ",
        "outputId": "1eaf1a95-f309-4242-f4a1-48e079d7f95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!saved_model_cli show --dir {export_path} --all"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-12 09:45:00.410813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['Conv1_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_Conv1_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['Softmax'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "The MetaGraph with tag set ['serve'] contains the following ops: {'Identity', 'ShardedFilename', 'Pack', 'Const', 'MergeV2Checkpoints', 'SaveV2', 'Placeholder', 'Reshape', 'NoOp', 'StaticRegexFullMatch', 'RestoreV2', 'MatMul', 'AssignVariableOp', 'ReadVariableOp', 'BiasAdd', 'DisableCopyOnRead', 'VarHandleOp', 'Conv2D', 'StatefulPartitionedCall', 'Select', 'Relu', 'StringJoin', 'Softmax'}\n",
            "2023-05-12 09:45:03.324412: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "\n",
            "Concrete Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDTdBGHZAzo"
      },
      "source": [
        "## Add TensorFlow Serving Distribution URI as a Package Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWg9X2QHlbGS",
        "outputId": "595fb00d-1f2e-49b2-93c8-0f7aaead15a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2943  100  2943    0     0   2561      0  0:00:01  0:00:01 --:--:--  2563\n",
            "OK\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,026 B]\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:9 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [348 B]\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:11 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [338 B]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,046 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:15 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,573 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,158 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,681 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,343 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,213 kB]\n",
            "Fetched 12.4 MB in 5s (2,348 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l5XkzqNZNBU"
      },
      "source": [
        "## Install TensorFlow Serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwa9AgRloYy",
        "outputId": "13fad963-8a93-4379-d423-3766696f32c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install tensorflow-model-server"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tensorflow-model-server\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.12.1 [430 MB]\n",
            "Fetched 430 MB in 23s (18.9 MB/s)\n",
            "Selecting previously unselected package tensorflow-model-server.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.12.1_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.12.1) ...\n",
            "Setting up tensorflow-model-server (2.12.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd_PobAKZWW8"
      },
      "source": [
        "## Run the TensorFlow Model Server\n",
        "\n",
        "You will now launch the TensorFlow model server with a bash script. In the cell below use the following parameters when running the TensorFlow model server:\n",
        "\n",
        "* `rest_api_port`: Use port `8501` for your requests.\n",
        "\n",
        "\n",
        "* `model_name`: Use `digits_model` as your model name. \n",
        "\n",
        "\n",
        "* `model_base_path`: Use the environment variable `MODEL_DIR` defined below as the base path to the saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUgp3vUdU5GS"
      },
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJDhHNJVnaLN"
      },
      "source": [
        "# EXERCISE: Fill in the missing code below.\n",
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port= 8501 \\\n",
        "  --model_name= digits_model \\\n",
        "  --model_base_path= \"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxbeiOCUUs2z",
        "outputId": "b8f923f1-8935-46e4-b57b-9851b425617d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t--version=false                  \tbool\tDisplay version\n",
            "\t--monitoring_config_file=\"\"      \tstring\tIf non-empty, read an ascii MonitoringConfig protobuf from the supplied file name\n",
            "\t--remove_unused_fields_from_bundle_metagraph=true\tbool\tRemoves unused fields from MetaGraphDef proto message to save memory.\n",
            "\t--prefer_tflite_model=false      \tbool\tEXPERIMENTAL; CAN BE REMOVED ANYTIME! Prefer TensorFlow Lite model from `model.tflite` file in SavedModel directory, instead of the TensorFlow model from `saved_model.pb` file. If no TensorFlow Lite model found, fallback to TensorFlow model.\n",
            "\t--num_tflite_pools=2             \tint32\tEXPERIMENTAL; CAN BE REMOVED ANYTIME! Number of TFLite interpreters in an interpreter pool of TfLiteSession. Typically there is one TfLiteSession for each TF Lite model that is loaded. If not set, will be auto set based on number of CPUs.\n",
            "\t--num_tflite_interpreters_per_pool=1\tint32\tEXPERIMENTAL; CAN BE REMOVED ANYTIME! Number of TFLite interpreters in an interpreter pool of TfLiteSession. Typically there is one TfLiteSession for each TF Lite model that is loaded. If not set, will be 1.\n",
            "\t--enable_signature_method_name_check=false\tbool\tEnable method_name check for SignatureDef. Disable this if serving native TF2 regression/classification models.\n",
            "\t--xla_cpu_compilation_enabled=false\tbool\tEXPERIMENTAL; CAN BE REMOVED ANYTIME! Enable XLA:CPU JIT (default is disabled). With XLA:CPU JIT disabled, models utilizing this feature will return bad Status on first compilation request.\n",
            "\t--enable_profiler=true           \tbool\tEnable profiler service.\n",
            "\t--thread_pool_factory_config_file=\"\"\tstring\tIf non-empty, read an ascii ThreadPoolConfig protobuf from the supplied file name.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mUrIWVRZdNu"
      },
      "source": [
        "## Create JSON Object with Test Images\n",
        "\n",
        "In the cell below construct a JSON object and use the first three images of the testing set (`test_images`) as your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dsD7KQG1m-R"
      },
      "source": [
        "# EXERCISE: Create JSON Object\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRdyPl4CZ5CU"
      },
      "source": [
        "## Make Inference Request\n",
        "\n",
        "In the cell below, send a predict request as a POST to the server's REST endpoint, and pass it your test data. You should ask the server to give you the latest version of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvFyuIzW6n6"
      },
      "source": [
        "# EXERCISE: Fill in the code below\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://192.168.1.1:8501/v1/models/digits_model:predict', data=data, headers=headers)\n",
        "\n",
        "predictions = json.loads(json_response.text)['predictions']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrFMts_ackX"
      },
      "source": [
        "## Plot Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxQzj34aiDz1"
      },
      "source": [
        "plt.figure(figsize=(10,15))\n",
        "\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(test_images[i].reshape(28,28), cmap = plt.cm.binary)\n",
        "    plt.axis('off')\n",
        "    color = 'green' if np.argmax(predictions[i]) == test_labels[i] else 'red'\n",
        "    plt.title('Prediction: {}\\nTrue Label: {}'.format(np.argmax(predictions[i]), test_labels[i]), color=color)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}